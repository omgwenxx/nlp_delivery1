{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries and set randome seed for reproducibility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn import *\n",
    "import os\n",
    "import pickle  # to save model\n",
    "from utils import *\n",
    "\n",
    "RANDOM_SEED = 123  # taken from task description\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "           id    qid1    qid2  \\\n0      123842  200121  106133   \n1      385214  517332  205076   \n2      291190  412570  412571   \n3      319338  329701  157107   \n4      377054  290027  508282   \n...       ...     ...     ...   \n80853  376451  507598  507599   \n80854  332528  459522  459523   \n80855  147020  232119   17978   \n80856   29775   55057   55058   \n80857   99328  164908  164909   \n\n                                               question1  \\\n0                What is the Burj Khalifa damper system?   \n1                      How can one open a demat account?   \n2      In what ways would you want to contribute some...   \n3            What are the best earphones within Rs 3000?   \n4            Which book is the best for GRE preparation?   \n...                                                  ...   \n80853         Are A grades enough to get into Cambridge?   \n80854                As of July 2015, how is Quip doing?   \n80855  What are some things new employees should know...   \n80856                       Which is best ISP in indore?   \n80857  Is $30,000 in bank account a lot for a 21 year...   \n\n                                               question2  is_duplicate  \n0      What are some mind blowing unseen images of Bu...             0  \n1                         How do I open a demat account?             1  \n2      Is it pragmatic to wish for a world free of vi...             0  \n3              Which is the best earphone under Rs 3000?             1  \n4      Which are the best books for the IELTS and the...             0  \n...                                                  ...           ...  \n80853       How do I get into Cambridge as a math major?             0  \n80854       How exactly does Quip work in layman's term?             0  \n80855  What are some things new employees should know...             0  \n80856                  Which is the best ISP in Chennai?             0  \n80857  Would a 30 year old women date a 21 year old man?             0  \n\n[80858 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>123842</td>\n      <td>200121</td>\n      <td>106133</td>\n      <td>What is the Burj Khalifa damper system?</td>\n      <td>What are some mind blowing unseen images of Bu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>385214</td>\n      <td>517332</td>\n      <td>205076</td>\n      <td>How can one open a demat account?</td>\n      <td>How do I open a demat account?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>291190</td>\n      <td>412570</td>\n      <td>412571</td>\n      <td>In what ways would you want to contribute some...</td>\n      <td>Is it pragmatic to wish for a world free of vi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>319338</td>\n      <td>329701</td>\n      <td>157107</td>\n      <td>What are the best earphones within Rs 3000?</td>\n      <td>Which is the best earphone under Rs 3000?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>377054</td>\n      <td>290027</td>\n      <td>508282</td>\n      <td>Which book is the best for GRE preparation?</td>\n      <td>Which are the best books for the IELTS and the...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>80853</th>\n      <td>376451</td>\n      <td>507598</td>\n      <td>507599</td>\n      <td>Are A grades enough to get into Cambridge?</td>\n      <td>How do I get into Cambridge as a math major?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>80854</th>\n      <td>332528</td>\n      <td>459522</td>\n      <td>459523</td>\n      <td>As of July 2015, how is Quip doing?</td>\n      <td>How exactly does Quip work in layman's term?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>80855</th>\n      <td>147020</td>\n      <td>232119</td>\n      <td>17978</td>\n      <td>What are some things new employees should know...</td>\n      <td>What are some things new employees should know...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>80856</th>\n      <td>29775</td>\n      <td>55057</td>\n      <td>55058</td>\n      <td>Which is best ISP in indore?</td>\n      <td>Which is the best ISP in Chennai?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>80857</th>\n      <td>99328</td>\n      <td>164908</td>\n      <td>164909</td>\n      <td>Is $30,000 in bank account a lot for a 21 year...</td>\n      <td>Would a 30 year old women date a 21 year old man?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>80858 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this to train and VALIDATE your solution\n",
    "train_df = pd.read_csv(\"./quora_train_data.csv\")\n",
    "\n",
    "# use this to provide the expected generalization results\n",
    "test_df = pd.read_csv(\"./quora_test_data.csv\")\n",
    "test_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<8x78275 sparse matrix of type '<class 'numpy.int64'>'\n\twith 7 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract questions (documents) and cast to strings\n",
    "q1_train =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "all_questions = q1_train + q2_train\n",
    "\n",
    "count_vectorizer_v1 = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer_v1.fit(all_questions)\n",
    "a = q1_train[0].lower().split()\n",
    "b = q2_train[0].lower().split()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_q1 = count_vectorizer_v1.transform([q1_train[0]])\n",
    "X_q1.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['get', 'bored', 'with', 'i', 'do', 'why']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(a)&set(b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = abs(len(a) - len(b))\n",
    "diff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def jaccard_similarity(a, b):\n",
    "    intersection = set(a).intersection(set(b))\n",
    "    union = set(a).union(set(b))\n",
    "    return len(intersection)/len(union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "0.46153846153846156"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity(a,b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def term_frequency(sentence, ignore_tokens=[\"<SOS>\",\"<EOS>\"], lower_case = False):\n",
    "    \"\"\"\n",
    "    Computes the term frequency for a single sentence. Used as auxiliary for other methods.\n",
    "    Arguments:\n",
    "    ----------\n",
    "    document: list of string.\n",
    "        The \"document\" to compute the global term frequency.\n",
    "    ignore_tokens: list of str.\n",
    "        Tokens to ignore in the term frequency computation.\n",
    "    lower_case: boolean.\n",
    "        Whether to be case sensitive or not. Defaults to False (case sensitive).\n",
    "    \"\"\"\n",
    "    word_dict = {}\n",
    "    # Simple preprocessing step\n",
    "    words = [token.lower() if lower_case else token for token in sentence.split() if token not in ignore_tokens]\n",
    "    for word in words:\n",
    "        word_dict[word] = word_dict.get(word, 0)+1\n",
    "    return word_dict\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class TFIDF:\n",
    "    def __init__(self, ignore_tokens=[\"<SOS>\",\"<EOS>\"], ignore_punctuation=True, lower_case = False):\n",
    "        self.ignore_tokens = ignore_tokens\n",
    "        if ignore_punctuation:\n",
    "            self.ignore_tokens += [char for char in string.punctuation]\n",
    "        self.lower_case = lower_case\n",
    "        self.word_indexes = {}\n",
    "        self.index_to_word = {}\n",
    "        # idf_dict will store D/freq(word) for each word. We'll use later for the end computation.\n",
    "        self.idf_dict = {}\n",
    "        self.num_documents = 0\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fits the data into the Featurizer.\n",
    "        Arguments\n",
    "        ---------\n",
    "        data: list of string.\n",
    "            The data to fit the featurizer.\n",
    "        Exceptions\n",
    "        ----------\n",
    "        TypeError\n",
    "            Related to data type. Expects list of strings.\n",
    "        \"\"\"\n",
    "        self.num_documents = len(data)\n",
    "        #compute global term frequency by iterating over all sentences and counting all words.\n",
    "        global_term_freq = {}\n",
    "        list_of_sentences = data\n",
    "        for sentence in list_of_sentences:\n",
    "            words_in_sent = set()\n",
    "            document_frequency = term_frequency(sentence, self.ignore_tokens, self.lower_case)\n",
    "            for word in document_frequency:\n",
    "                if word not in words_in_sent:\n",
    "                    global_term_freq[word] = global_term_freq.get(word, 0)+1\n",
    "                    words_in_sent.add(word)\n",
    "        # Compute idf for each word by finding the log of total num of docs divided by the total number of times a word appears in at least one document.\n",
    "        # Add 1 to numerator and denominator to avoid division by zero.\n",
    "        for word, frequency in global_term_freq.items():\n",
    "            idf = math.log(float(1 + self.num_documents) / (1 + frequency))\n",
    "            self.idf_dict[word]=idf\n",
    "        # Initialize the indexer\n",
    "        document_words = list(global_term_freq.keys())\n",
    "        for word_position in range(len(document_words)):\n",
    "            word = document_words[word_position]\n",
    "            self.word_indexes[word] = word_position\n",
    "            self.index_to_word[word_position] = word\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Transforms the data passed as input into a tdf-idf vector/matrix, depending on the input.\n",
    "        Arguments\n",
    "        ---------\n",
    "        data: list of string or string.\n",
    "            The data to fit the featurizer.\n",
    "        AttributeError\n",
    "            Related to the vocabulary lenght. Happens if fit with empty data or not fit.\n",
    "        \"\"\"\n",
    "        if isinstance(data, list):\n",
    "            return self._transform_document(data)\n",
    "        elif isinstance(data, str):\n",
    "            return self._transform_sentence(data)\n",
    "\n",
    "    def _transform_document(self, data):\n",
    "        \"\"\" This method is just used for simple batch transforming. \"\"\"\n",
    "        to_transform = data\n",
    "        sentence_arrays = []\n",
    "        for sentence in data:\n",
    "            sentence_arrays.append(self._transform_sentence(sentence))\n",
    "        return np.matrix(sentence_arrays)\n",
    "\n",
    "    def _transform_sentence(self, data):\n",
    "        tokens = [token.lower() if self.lower_case else token for token in data.split()]\n",
    "        # Initializes array with the size of vocabulary.\n",
    "        word_array = np.zeros(len(self.word_indexes))\n",
    "        sentence_tf_idf = self._compute_sentence_tf_idf(data)\n",
    "        # Runs over every token in sentence\n",
    "        for token in tokens:\n",
    "            if token in self.word_indexes:\n",
    "                token_index = self.word_indexes[token]\n",
    "                # Add the tfidf value for each token in sentence to its position in vocabulary array.\n",
    "                word_array[token_index] = sentence_tf_idf[token]\n",
    "        return word_array\n",
    "\n",
    "    def _compute_sentence_tf_idf(self, sentence):\n",
    "        \"\"\"\n",
    "        Computes the tf_idf for a single sentence(document).\n",
    "        \"\"\"\n",
    "        sentence_tf_idf = {}\n",
    "        # Gets the document frequency by using the helper method\n",
    "        document_frequency = term_frequency(sentence, self.ignore_tokens, self.lower_case)\n",
    "        # Gets the total number of words in sentence\n",
    "        total_words = sum(document_frequency.values())\n",
    "        # Find individual term frequency value averaged by total number of words.\n",
    "        averaged_frequency = {k:(float(v)/total_words) for k,v in document_frequency.items()}\n",
    "        for term, tf in averaged_frequency.items():\n",
    "            # Out of vocabulary words are simply zeroed. They are going to be removed later either way.\n",
    "            # Computes the tfidf for each word by using word tf times the term idf\n",
    "            sentence_tf_idf[term] = tf*self.idf_dict.get(term, 0)\n",
    "        return sentence_tf_idf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "matrix([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['this is a list of sentences', 'second sentence in list of sentences', 'a word for complexity']\n",
    "featurizer = TFIDF()\n",
    "featurizer.fit(all_questions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q1_tfidf = featurizer.transform(q1_train)\n",
    "q2_tfidf = featurizer.transform(q2_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}