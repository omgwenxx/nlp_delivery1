{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7b25c7",
   "metadata": {},
   "source": [
    "This notebook is to train and store models to disc.\\\n",
    "This Notebook has to be clean (do not define functions here, do them in an\n",
    "external utils.py and import them).\\\n",
    "This notebook has to be reproducible (if you run it twice, the same output has to\n",
    "be displayed and stored to disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f8a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and set randome seed for reproducibility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn import *\n",
    "import os\n",
    "import pickle # to save model\n",
    "from utils import *\n",
    "RANDOM_SEED = 123 # taken from task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39568d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to train and VALIDATE your solution\n",
    "train_df = pd.read_csv(\"./quora_train_data.csv\")\n",
    "\n",
    "# use this to provide the expected generalization results\n",
    "test_df = pd.read_csv(\"./quora_test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8784dd",
   "metadata": {},
   "source": [
    "We need to convert our questions to strings in order to work with CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3309e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((323432, 156550), (323432, 6), (80858, 156550), (80858, 6))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract questions (documents) and cast to strings\n",
    "q1_train =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "all_questions = q1_train + q2_train\n",
    "\n",
    "# fit on train set\n",
    "count_vectorizer_v1 = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer_v1.fit(all_questions)\n",
    "\n",
    "X_tr_q1q2 = get_features_from_df(train_df,count_vectorizer_v1)\n",
    "X_te_q1q2  = get_features_from_df(test_df, count_vectorizer_v1)\n",
    "\n",
    "X_tr_q1q2.shape, train_df.shape, X_te_q1q2.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717056e0",
   "metadata": {},
   "source": [
    "Divide processed train set into train and validation according to task description split. Using `random_seed = 123`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b65da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_df.shape= (307260, 156550)\n",
      "va_df.shape= (16172, 156550)\n",
      "te_df.shape= (80858, 156550)\n",
      "y_train.shape= (307260,)\n",
      "y_val.shape= (16172,)\n"
     ]
    }
   ],
   "source": [
    "# divide into train and validation set\n",
    "tr_df, va_df, y_train, y_val = sklearn.model_selection.train_test_split(X_tr_q1q2, train_df[\"is_duplicate\"].values, test_size=0.05, random_state=RANDOM_SEED)\n",
    "print('tr_df.shape=',tr_df.shape)\n",
    "print('va_df.shape=',va_df.shape)\n",
    "print('te_df.shape=',X_te_q1q2.shape)\n",
    "print('y_train.shape=',y_train.shape)\n",
    "print('y_val.shape=',y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa3f3e",
   "metadata": {},
   "source": [
    "Train model on count vectorized matrix of question1 and question2 using train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4906a409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=123, solver='liblinear')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\",random_state=RANDOM_SEED)\n",
    "logistic.fit(tr_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca3ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "save_model(\"logreg\",logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b77ddde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results\n",
      "Val ROC-AUC: 0.727\n",
      "\n",
      "Test Results\n",
      "Test ROC-AUC: 0.724\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Validation\n",
    "print(\"Validation Results\")\n",
    "predictions = logistic.predict(va_df)\n",
    "result = roc_auc_score(y_val, predictions)\n",
    "print(\"Val ROC-AUC: %.3f\"%(result))\n",
    "      \n",
    "# Test   \n",
    "print(\"\\nTest Results\")\n",
    "predictions = logistic.predict(X_te_q1q2)\n",
    "result = roc_auc_score(test_df[\"is_duplicate\"].values, predictions)\n",
    "print(\"Test ROC-AUC: %.3f\"%(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a95ff401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why do i get easily bored with everything?\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt') # download if not exist\n",
    "# nltk.download('stopwords') # download if not exist\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess(sentence):\n",
    "    \"\"\"\n",
    "    Function to preprocess sentences in question1 and question2 column. First words are lower cased,\n",
    "    then punctuation and stop words (using nltk library) are removed. Then we stem the words\n",
    "    to remove pre- or postfixes.\n",
    "    \"\"\"\n",
    "    text = sentence.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    print(text)\n",
    "    text = text.lower() # lower case\n",
    "    print(text)\n",
    "    text_tokens = word_tokenize(text) # tokenizing words\n",
    "    print(text_tokens)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()] # remove stop words\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokens_stem = [ps.stem(word) for word in tokens_without_sw]\n",
    "    return tokens_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bc6f27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do I get easily bored with everything\n",
      "why do i get easily bored with everything\n",
      "['why', 'do', 'i', 'get', 'easily', 'bored', 'with', 'everything']\n",
      "['get', 'easili', 'bore', 'everyth']\n",
      "How do I study for Honeywell company recruitment\n",
      "how do i study for honeywell company recruitment\n",
      "['how', 'do', 'i', 'study', 'for', 'honeywell', 'company', 'recruitment']\n",
      "['studi', 'honeywel', 'compani', 'recruit']\n",
      "Which search engine algorithm is Quora using\n",
      "which search engine algorithm is quora using\n",
      "['which', 'search', 'engine', 'algorithm', 'is', 'quora', 'using']\n",
      "['search', 'engin', 'algorithm', 'quora', 'use']\n",
      "How can I smartly cut myself\n",
      "how can i smartly cut myself\n",
      "['how', 'can', 'i', 'smartly', 'cut', 'myself']\n",
      "['smartli', 'cut']\n",
      "How do I see who is viewing my Instagram videos\n",
      "how do i see who is viewing my instagram videos\n",
      "['how', 'do', 'i', 'see', 'who', 'is', 'viewing', 'my', 'instagram', 'videos']\n",
      "['see', 'view', 'instagram', 'video']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(preprocess(train_df[\"question1\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fc72e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
